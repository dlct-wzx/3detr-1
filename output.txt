Could not import cythonized box intersection. Consider compiling box_intersection.pyx for faster training.
Cannot import tensorboard. Will log to txt files only.
Called with args: Namespace(base_lr=0.0005, batchsize_per_gpu=8, checkpoint_dir='log_to_scene', clip_gradient=0.1, dataset_name='to_scene', dataset_num_workers=4, dataset_root_dir='to_scene', dec_dim=256, dec_dropout=0.1, dec_ffn_dim=256, dec_nhead=4, dec_nlayers=8, dist_url='tcp://localhost:12345', enc_activation='relu', enc_dim=256, enc_dropout=0.1, enc_ffn_dim=128, enc_nhead=4, enc_nlayers=3, enc_pos_embed=None, enc_type='vanilla', eval_every_epoch=10, filter_biases_wd=False, final_lr=1e-06, log_every=10, log_metrics_every=20, loss_angle_cls_weight=0.1, loss_angle_reg_weight=0.5, loss_center_weight=5.0, loss_giou_weight=0, loss_no_object_weight=0.2, loss_sem_cls_weight=1, loss_size_weight=1.0, lr_scheduler='cosine', matcher_center_cost=0, matcher_cls_cost=1, matcher_giou_cost=2, matcher_objectness_cost=0, max_epoch=720, meta_data_dir=None, mlp_dropout=0.3, model_name='3detr', ngpus=1, nqueries=256, nsemcls=-1, pos_embed='fourier', preenc_npoints=2048, save_separate_checkpoint_every_epoch=100, seed=0, start_epoch=-1, test_ckpt=None, test_only=False, use_color=False, warm_lr=1e-06, warm_lr_epochs=9, weight_decay=0.1)
Model is Model3DETR(
  (pre_encoder): PointnetSAModuleVotes(
    (grouper): QueryAndGroup()
    (mlp_module): SharedMLP(
      (layer0): Conv2d(
        (conv): Conv2d(3, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn): BatchNorm2d(
          (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (activation): ReLU(inplace=True)
      )
      (layer1): Conv2d(
        (conv): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn): BatchNorm2d(
          (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (activation): ReLU(inplace=True)
      )
      (layer2): Conv2d(
        (conv): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn): BatchNorm2d(
          (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (activation): ReLU(inplace=True)
      )
    )
  )
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0): TransformerEncoderLayer(
        attn_dr=0.1
        (self_attn): MultiheadAttention(
          (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
        )
        (linear1): Linear(in_features=256, out_features=128, bias=True)
        (dropout): Dropout(p=0.1, inplace=True)
        (linear2): Linear(in_features=128, out_features=256, bias=True)
        (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout2): Dropout(p=0.1, inplace=True)
        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=True)
        (activation): ReLU()
      )
      (1): TransformerEncoderLayer(
        attn_dr=0.1
        (self_attn): MultiheadAttention(
          (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
        )
        (linear1): Linear(in_features=256, out_features=128, bias=True)
        (dropout): Dropout(p=0.1, inplace=True)
        (linear2): Linear(in_features=128, out_features=256, bias=True)
        (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout2): Dropout(p=0.1, inplace=True)
        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=True)
        (activation): ReLU()
      )
      (2): TransformerEncoderLayer(
        attn_dr=0.1
        (self_attn): MultiheadAttention(
          (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
        )
        (linear1): Linear(in_features=256, out_features=128, bias=True)
        (dropout): Dropout(p=0.1, inplace=True)
        (linear2): Linear(in_features=128, out_features=256, bias=True)
        (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout2): Dropout(p=0.1, inplace=True)
        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=True)
        (activation): ReLU()
      )
    )
  )
  (encoder_to_decoder_projection): GenericMLP(
    (layers): Sequential(
      (0): Conv1d(256, 256, kernel_size=(1,), stride=(1,), bias=False)
      (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU()
      (3): Conv1d(256, 256, kernel_size=(1,), stride=(1,), bias=False)
      (4): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (5): ReLU()
      (6): Conv1d(256, 256, kernel_size=(1,), stride=(1,), bias=False)
      (7): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (8): ReLU()
    )
  )
  (pos_embedding): PositionEmbeddingCoordsSine(type=fourier, scale=6.283185307179586, normalize=True, gaussB=torch.Size([3, 128]), gaussBsum=-12.79810905456543)
  (query_projection): GenericMLP(
    (layers): Sequential(
      (0): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
      (1): ReLU()
      (2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
      (3): ReLU()
    )
  )
  (decoder): TransformerDecoder(
    (layers): ModuleList(
      (0): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
        )
        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=True)
        (dropout2): Dropout(p=0.1, inplace=True)
        (dropout3): Dropout(p=0.1, inplace=True)
        (linear1): Linear(in_features=256, out_features=256, bias=True)
        (dropout): Dropout(p=0.1, inplace=True)
        (linear2): Linear(in_features=256, out_features=256, bias=True)
        (activation): ReLU()
      )
      (1): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
        )
        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=True)
        (dropout2): Dropout(p=0.1, inplace=True)
        (dropout3): Dropout(p=0.1, inplace=True)
        (linear1): Linear(in_features=256, out_features=256, bias=True)
        (dropout): Dropout(p=0.1, inplace=True)
        (linear2): Linear(in_features=256, out_features=256, bias=True)
        (activation): ReLU()
      )
      (2): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
        )
        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=True)
        (dropout2): Dropout(p=0.1, inplace=True)
        (dropout3): Dropout(p=0.1, inplace=True)
        (linear1): Linear(in_features=256, out_features=256, bias=True)
        (dropout): Dropout(p=0.1, inplace=True)
        (linear2): Linear(in_features=256, out_features=256, bias=True)
        (activation): ReLU()
      )
      (3): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
        )
        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=True)
        (dropout2): Dropout(p=0.1, inplace=True)
        (dropout3): Dropout(p=0.1, inplace=True)
        (linear1): Linear(in_features=256, out_features=256, bias=True)
        (dropout): Dropout(p=0.1, inplace=True)
        (linear2): Linear(in_features=256, out_features=256, bias=True)
        (activation): ReLU()
      )
      (4): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
        )
        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=True)
        (dropout2): Dropout(p=0.1, inplace=True)
        (dropout3): Dropout(p=0.1, inplace=True)
        (linear1): Linear(in_features=256, out_features=256, bias=True)
        (dropout): Dropout(p=0.1, inplace=True)
        (linear2): Linear(in_features=256, out_features=256, bias=True)
        (activation): ReLU()
      )
      (5): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
        )
        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=True)
        (dropout2): Dropout(p=0.1, inplace=True)
        (dropout3): Dropout(p=0.1, inplace=True)
        (linear1): Linear(in_features=256, out_features=256, bias=True)
        (dropout): Dropout(p=0.1, inplace=True)
        (linear2): Linear(in_features=256, out_features=256, bias=True)
        (activation): ReLU()
      )
      (6): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
        )
        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=True)
        (dropout2): Dropout(p=0.1, inplace=True)
        (dropout3): Dropout(p=0.1, inplace=True)
        (linear1): Linear(in_features=256, out_features=256, bias=True)
        (dropout): Dropout(p=0.1, inplace=True)
        (linear2): Linear(in_features=256, out_features=256, bias=True)
        (activation): ReLU()
      )
      (7): TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): _LinearWithBias(in_features=256, out_features=256, bias=True)
        )
        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=True)
        (dropout2): Dropout(p=0.1, inplace=True)
        (dropout3): Dropout(p=0.1, inplace=True)
        (linear1): Linear(in_features=256, out_features=256, bias=True)
        (dropout): Dropout(p=0.1, inplace=True)
        (linear2): Linear(in_features=256, out_features=256, bias=True)
        (activation): ReLU()
      )
    )
    (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
  )
  (mlp_heads): ModuleDict(
    (sem_cls_head): GenericMLP(
      (layers): Sequential(
        (0): Conv1d(256, 256, kernel_size=(1,), stride=(1,), bias=False)
        (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU()
        (3): Dropout(p=0.3, inplace=False)
        (4): Conv1d(256, 256, kernel_size=(1,), stride=(1,), bias=False)
        (5): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (6): ReLU()
        (7): Dropout(p=0.3, inplace=False)
        (8): Conv1d(256, 54, kernel_size=(1,), stride=(1,))
      )
    )
    (center_head): GenericMLP(
      (layers): Sequential(
        (0): Conv1d(256, 256, kernel_size=(1,), stride=(1,), bias=False)
        (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU()
        (3): Dropout(p=0.3, inplace=False)
        (4): Conv1d(256, 256, kernel_size=(1,), stride=(1,), bias=False)
        (5): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (6): ReLU()
        (7): Dropout(p=0.3, inplace=False)
        (8): Conv1d(256, 3, kernel_size=(1,), stride=(1,))
      )
    )
    (size_head): GenericMLP(
      (layers): Sequential(
        (0): Conv1d(256, 256, kernel_size=(1,), stride=(1,), bias=False)
        (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU()
        (3): Dropout(p=0.3, inplace=False)
        (4): Conv1d(256, 256, kernel_size=(1,), stride=(1,), bias=False)
        (5): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (6): ReLU()
        (7): Dropout(p=0.3, inplace=False)
        (8): Conv1d(256, 3, kernel_size=(1,), stride=(1,))
      )
    )
    (angle_cls_head): GenericMLP(
      (layers): Sequential(
        (0): Conv1d(256, 256, kernel_size=(1,), stride=(1,), bias=False)
        (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU()
        (3): Dropout(p=0.3, inplace=False)
        (4): Conv1d(256, 256, kernel_size=(1,), stride=(1,), bias=False)
        (5): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (6): ReLU()
        (7): Dropout(p=0.3, inplace=False)
        (8): Conv1d(256, 12, kernel_size=(1,), stride=(1,))
      )
    )
    (angle_residual_head): GenericMLP(
      (layers): Sequential(
        (0): Conv1d(256, 256, kernel_size=(1,), stride=(1,), bias=False)
        (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU()
        (3): Dropout(p=0.3, inplace=False)
        (4): Conv1d(256, 256, kernel_size=(1,), stride=(1,), bias=False)
        (5): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (6): ReLU()
        (7): Dropout(p=0.3, inplace=False)
        (8): Conv1d(256, 12, kernel_size=(1,), stride=(1,))
      )
    )
  )
)
Training started at epoch 0 until 720.
One training epoch = 1110 iters.
One eval epoch = 249 iters.
Max index in gt_box_sem_cls_label: 51
Min index in gt_box_sem_cls_label: 0
Max index in gt_box_sem_cls_label: 51
Min index in gt_box_sem_cls_label: 0
Max index in gt_box_sem_cls_label: 51
Min index in gt_box_sem_cls_label: 0
Max index in gt_box_sem_cls_label: 51
Min index in gt_box_sem_cls_label: 0
Max index in gt_box_sem_cls_label: 51
Min index in gt_box_sem_cls_label: 0
Max index in gt_box_sem_cls_label: 51
Min index in gt_box_sem_cls_label: 0
Max index in gt_box_sem_cls_label: 51
Min index in gt_box_sem_cls_label: 0
Max index in gt_box_sem_cls_label: 51
Min index in gt_box_sem_cls_label: 0
Epoch [0/720]; Iter [0/799200]; Loss 63.85; LR 1.00e-06; Iter time 3.92; ETA 36 days, 5:17:07; Mem 12669.09MB
Max index in gt_box_sem_cls_label: 51
Min index in gt_box_sem_cls_label: 0
Max index in gt_box_sem_cls_label: 51
Min index in gt_box_sem_cls_label: 0
Max index in gt_box_sem_cls_label: 51
Min index in gt_box_sem_cls_label: 0
Max index in gt_box_sem_cls_label: 51
Min index in gt_box_sem_cls_label: 0
Max index in gt_box_sem_cls_label: 51
Min index in gt_box_sem_cls_label: 0
Max index in gt_box_sem_cls_label: 51
Min index in gt_box_sem_cls_label: 0
Max index in gt_box_sem_cls_label: 51
Min index in gt_box_sem_cls_label: 0
Max index in gt_box_sem_cls_label: 51
Min index in gt_box_sem_cls_label: 0
Max index in gt_box_sem_cls_label: 51
Min index in gt_box_sem_cls_label: 0
Max index in gt_box_sem_cls_label: 51
Min index in gt_box_sem_cls_label: 0
Max index in gt_box_sem_cls_label: 51
Min index in gt_box_sem_cls_label: 0
Max index in gt_box_sem_cls_label: 51
Min index in gt_box_sem_cls_label: 0
Max index in gt_box_sem_cls_label: 51
Min index in gt_box_sem_cls_label: 0
Max index in gt_box_sem_cls_label: 51
Min index in gt_box_sem_cls_label: 0
Max index in gt_box_sem_cls_label: 51
Min index in gt_box_sem_cls_label: 0
Max index in gt_box_sem_cls_label: 51
Min index in gt_box_sem_cls_label: 0
Max index in gt_box_sem_cls_label: 51
Min index in gt_box_sem_cls_label: 0
Max index in gt_box_sem_cls_label: 51
Min index in gt_box_sem_cls_label: 0
Max index in gt_box_sem_cls_label: 51
Min index in gt_box_sem_cls_label: 0
Max index in gt_box_sem_cls_label: 51
Min index in gt_box_sem_cls_label: 0
Max index in gt_box_sem_cls_label: 51
Min index in gt_box_sem_cls_label: 0
Max index in gt_box_sem_cls_label: 51
Min index in gt_box_sem_cls_label: 0
Max index in gt_box_sem_cls_label: 51
Min index in gt_box_sem_cls_label: 0
Max index in gt_box_sem_cls_label: 51
Min index in gt_box_sem_cls_label: 0
Max index in gt_box_sem_cls_label: 51
Min index in gt_box_sem_cls_label: 0
Max index in gt_box_sem_cls_label: 51
Min index in gt_box_sem_cls_label: 0
Max index in gt_box_sem_cls_label: 51
Min index in gt_box_sem_cls_label: 0
Max index in gt_box_sem_cls_label: 51
Min index in gt_box_sem_cls_label: 0
Max index in gt_box_sem_cls_label: 51
Min index in gt_box_sem_cls_label: 0
Max index in gt_box_sem_cls_label: 51
Min index in gt_box_sem_cls_label: 0
Max index in gt_box_sem_cls_label: 51
Min index in gt_box_sem_cls_label: 0
Max index in gt_box_sem_cls_label: 51
Min index in gt_box_sem_cls_label: 0
Max index in gt_box_sem_cls_label: 48
Min index in gt_box_sem_cls_label: 0
Max index in gt_box_sem_cls_label: 48
Min index in gt_box_sem_cls_label: 0
Max index in gt_box_sem_cls_label: 48
Min index in gt_box_sem_cls_label: 0
Max index in gt_box_sem_cls_label: 48
Min index in gt_box_sem_cls_label: 0
Max index in gt_box_sem_cls_label: 48
Min index in gt_box_sem_cls_label: 0
Max index in gt_box_sem_cls_label: 48
Min index in gt_box_sem_cls_label: 0
Max index in gt_box_sem_cls_label: 48
Min index in gt_box_sem_cls_label: 0
Max index in gt_box_sem_cls_label: 48
Min index in gt_box_sem_cls_label: 0
Max index in gt_box_sem_cls_label: 51
Min index in gt_box_sem_cls_label: 0
Max index in gt_box_sem_cls_label: 51
Min index in gt_box_sem_cls_label: 0
Max index in gt_box_sem_cls_label: 51
Min index in gt_box_sem_cls_label: 0
Max index in gt_box_sem_cls_label: 51
Min index in gt_box_sem_cls_label: 0
Max index in gt_box_sem_cls_label: 51
Min index in gt_box_sem_cls_label: 0
Max index in gt_box_sem_cls_label: 51
Min index in gt_box_sem_cls_label: 0
Max index in gt_box_sem_cls_label: 51
Min index in gt_box_sem_cls_label: 0
Max index in gt_box_sem_cls_label: 51
Min index in gt_box_sem_cls_label: 0
Max index in gt_box_sem_cls_label: 48
Min index in gt_box_sem_cls_label: 0
Max index in gt_box_sem_cls_label: 48
Min index in gt_box_sem_cls_label: 0
Max index in gt_box_sem_cls_label: 48
Min index in gt_box_sem_cls_label: 0
Max index in gt_box_sem_cls_label: 48
Min index in gt_box_sem_cls_label: 0
Max index in gt_box_sem_cls_label: 48
Min index in gt_box_sem_cls_label: 0
Max index in gt_box_sem_cls_label: 48
Min index in gt_box_sem_cls_label: 0
Max index in gt_box_sem_cls_label: 48
Min index in gt_box_sem_cls_label: 0
Max index in gt_box_sem_cls_label: 48
Min index in gt_box_sem_cls_label: 0
Max index in gt_box_sem_cls_label: 51
Min index in gt_box_sem_cls_label: 0
Max index in gt_box_sem_cls_label: 51
Min index in gt_box_sem_cls_label: 0
Max index in gt_box_sem_cls_label: 51
Min index in gt_box_sem_cls_label: 0
Max index in gt_box_sem_cls_label: 51
Min index in gt_box_sem_cls_label: 0
Max index in gt_box_sem_cls_label: 51
Min index in gt_box_sem_cls_label: 0
Max index in gt_box_sem_cls_label: 51
Min index in gt_box_sem_cls_label: 0
Max index in gt_box_sem_cls_label: 51
Min index in gt_box_sem_cls_label: 0
Max index in gt_box_sem_cls_label: 51
Min index in gt_box_sem_cls_label: 0
Max index in gt_box_sem_cls_label: 51
Min index in gt_box_sem_cls_label: 0
Max index in gt_box_sem_cls_label: 51
Min index in gt_box_sem_cls_label: 0
Max index in gt_box_sem_cls_label: 51
Min index in gt_box_sem_cls_label: 0
Max index in gt_box_sem_cls_label: 51
Min index in gt_box_sem_cls_label: 0
Max index in gt_box_sem_cls_label: 51
Min index in gt_box_sem_cls_label: 0
Max index in gt_box_sem_cls_label: 51
Min index in gt_box_sem_cls_label: 0
Max index in gt_box_sem_cls_label: 51
Min index in gt_box_sem_cls_label: 0
Max index in gt_box_sem_cls_label: 51
Min index in gt_box_sem_cls_label: 0
Max index in gt_box_sem_cls_label: 51
Min index in gt_box_sem_cls_label: 0
Max index in gt_box_sem_cls_label: 51
Min index in gt_box_sem_cls_label: 0
Max index in gt_box_sem_cls_label: 51
Min index in gt_box_sem_cls_label: 0
Max index in gt_box_sem_cls_label: 51
Min index in gt_box_sem_cls_label: 0
Max index in gt_box_sem_cls_label: 51
Min index in gt_box_sem_cls_label: 0
Max index in gt_box_sem_cls_label: 51
Min index in gt_box_sem_cls_label: 0
Max index in gt_box_sem_cls_label: 51
Min index in gt_box_sem_cls_label: 0
Max index in gt_box_sem_cls_label: 51
Min index in gt_box_sem_cls_label: 0
Epoch [0/720]; Iter [10/799200]; Loss 60.87; LR 1.50e-06; Iter time 3.06; ETA 28 days, 7:53:46; Mem 12775.11MB
Max index in gt_box_sem_cls_label: 51
Min index in gt_box_sem_cls_label: 0
Max index in gt_box_sem_cls_label: 51
Min index in gt_box_sem_cls_label: 0
Max index in gt_box_sem_cls_label: 51
Min index in gt_box_sem_cls_label: 0
Max index in gt_box_sem_cls_label: 51
Min index in gt_box_sem_cls_label: 0
Max index in gt_box_sem_cls_label: 51
Min index in gt_box_sem_cls_label: 0
Max index in gt_box_sem_cls_label: 51
Min index in gt_box_sem_cls_label: 0
Max index in gt_box_sem_cls_label: 51
Min index in gt_box_sem_cls_label: 0
Max index in gt_box_sem_cls_label: 51
Min index in gt_box_sem_cls_label: 0
Max index in gt_box_sem_cls_label: 51
Min index in gt_box_sem_cls_label: 0
Max index in gt_box_sem_cls_label: 51
Min index in gt_box_sem_cls_label: 0
Max index in gt_box_sem_cls_label: 51
Min index in gt_box_sem_cls_label: 0
Max index in gt_box_sem_cls_label: 51
Min index in gt_box_sem_cls_label: 0
Max index in gt_box_sem_cls_label: 51
Min index in gt_box_sem_cls_label: 0
Max index in gt_box_sem_cls_label: 51
Min index in gt_box_sem_cls_label: 0
Max index in gt_box_sem_cls_label: 51
Min index in gt_box_sem_cls_label: 0
Max index in gt_box_sem_cls_label: 51
Min index in gt_box_sem_cls_label: 0
Max index in gt_box_sem_cls_label: 51
Min index in gt_box_sem_cls_label: 0
Max index in gt_box_sem_cls_label: 51
Min index in gt_box_sem_cls_label: 0
Could not import cythonized box intersection. Consider compiling box_intersection.pyx for faster training.
Cannot import tensorboard. Will log to txt files only.
Could not import cythonized box intersection. Consider compiling box_intersection.pyx for faster training.
Cannot import tensorboard. Will log to txt files only.
Could not import cythonized box intersection. Consider compiling box_intersection.pyx for faster training.
Cannot import tensorboard. Will log to txt files only.
Could not import cythonized box intersection. Consider compiling box_intersection.pyx for faster training.
Cannot import tensorboard. Will log to txt files only.
Max index in gt_box_sem_cls_label: 51
Min index in gt_box_sem_cls_label: 0
Max index in gt_box_sem_cls_label: 51
Min index in gt_box_sem_cls_label: 0
Max index in gt_box_sem_cls_label: 51
Min index in gt_box_sem_cls_label: 0
Max index in gt_box_sem_cls_label: 51
Min index in gt_box_sem_cls_label: 0
Max index in gt_box_sem_cls_label: 51
Min index in gt_box_sem_cls_label: 0
Max index in gt_box_sem_cls_label: 51
Min index in gt_box_sem_cls_label: 0
Max index in gt_box_sem_cls_label: 45
Min index in gt_box_sem_cls_label: 0
Max index in gt_box_sem_cls_label: 45
Min index in gt_box_sem_cls_label: 0
Max index in gt_box_sem_cls_label: 45
Min index in gt_box_sem_cls_label: 0
Max index in gt_box_sem_cls_label: 45
Min index in gt_box_sem_cls_label: 0
Max index in gt_box_sem_cls_label: 45
Min index in gt_box_sem_cls_label: 0
Max index in gt_box_sem_cls_label: 45
Min index in gt_box_sem_cls_label: 0
Max index in gt_box_sem_cls_label: 45
Min index in gt_box_sem_cls_label: 0
Max index in gt_box_sem_cls_label: 45
Min index in gt_box_sem_cls_label: 0
Max index in gt_box_sem_cls_label: 48
Min index in gt_box_sem_cls_label: 0
Max index in gt_box_sem_cls_label: 48
Min index in gt_box_sem_cls_label: 0
Max index in gt_box_sem_cls_label: 48
Min index in gt_box_sem_cls_label: 0
Max index in gt_box_sem_cls_label: 48
Min index in gt_box_sem_cls_label: 0
Max index in gt_box_sem_cls_label: 48
Min index in gt_box_sem_cls_label: 0
Max index in gt_box_sem_cls_label: 48
Min index in gt_box_sem_cls_label: 0
Max index in gt_box_sem_cls_label: 48
Min index in gt_box_sem_cls_label: 0
Max index in gt_box_sem_cls_label: 48
Min index in gt_box_sem_cls_label: 0
Max index in gt_box_sem_cls_label: 51
Min index in gt_box_sem_cls_label: 0
Max index in gt_box_sem_cls_label: 51
Min index in gt_box_sem_cls_label: 0
Max index in gt_box_sem_cls_label: 51
Min index in gt_box_sem_cls_label: 0
Max index in gt_box_sem_cls_label: 51
Min index in gt_box_sem_cls_label: 0
Max index in gt_box_sem_cls_label: 51
Min index in gt_box_sem_cls_label: 0
Max index in gt_box_sem_cls_label: 51
Min index in gt_box_sem_cls_label: 0
Max index in gt_box_sem_cls_label: 51
Min index in gt_box_sem_cls_label: 0
Max index in gt_box_sem_cls_label: 51
Min index in gt_box_sem_cls_label: 0
Max index in gt_box_sem_cls_label: 51
Min index in gt_box_sem_cls_label: 0
Max index in gt_box_sem_cls_label: 51
Min index in gt_box_sem_cls_label: 0
Max index in gt_box_sem_cls_label: 51
Min index in gt_box_sem_cls_label: 0
Max index in gt_box_sem_cls_label: 51
Min index in gt_box_sem_cls_label: 0
Max index in gt_box_sem_cls_label: 51
Min index in gt_box_sem_cls_label: 0
Max index in gt_box_sem_cls_label: 51
Min index in gt_box_sem_cls_label: 0
Max index in gt_box_sem_cls_label: 51
Min index in gt_box_sem_cls_label: 0
Max index in gt_box_sem_cls_label: 51
Min index in gt_box_sem_cls_label: 0
Max index in gt_box_sem_cls_label: 51
Min index in gt_box_sem_cls_label: 0
Max index in gt_box_sem_cls_label: 51
Min index in gt_box_sem_cls_label: 0
Max index in gt_box_sem_cls_label: 51
Min index in gt_box_sem_cls_label: 0
Max index in gt_box_sem_cls_label: 51
Min index in gt_box_sem_cls_label: 0
Max index in gt_box_sem_cls_label: 51
Min index in gt_box_sem_cls_label: 0
Max index in gt_box_sem_cls_label: 51
Min index in gt_box_sem_cls_label: 0
Max index in gt_box_sem_cls_label: 51
Min index in gt_box_sem_cls_label: 0
Max index in gt_box_sem_cls_label: 51
Min index in gt_box_sem_cls_label: 0
Max index in gt_box_sem_cls_label: 51
Min index in gt_box_sem_cls_label: 0
Max index in gt_box_sem_cls_label: 51
Min index in gt_box_sem_cls_label: 0
Max index in gt_box_sem_cls_label: 51
Min index in gt_box_sem_cls_label: 0
Max index in gt_box_sem_cls_label: 51
Min index in gt_box_sem_cls_label: 0
Max index in gt_box_sem_cls_label: 51
Min index in gt_box_sem_cls_label: 0
Max index in gt_box_sem_cls_label: 51
Min index in gt_box_sem_cls_label: 0
Max index in gt_box_sem_cls_label: 51
Min index in gt_box_sem_cls_label: 0
Max index in gt_box_sem_cls_label: 51
Min index in gt_box_sem_cls_label: 0
Max index in gt_box_sem_cls_label: 51
Min index in gt_box_sem_cls_label: 0
Max index in gt_box_sem_cls_label: 51
Min index in gt_box_sem_cls_label: 0
Max index in gt_box_sem_cls_label: 51
Min index in gt_box_sem_cls_label: 0
Max index in gt_box_sem_cls_label: 51
Min index in gt_box_sem_cls_label: 0
Max index in gt_box_sem_cls_label: 51
Min index in gt_box_sem_cls_label: 0
Max index in gt_box_sem_cls_label: 51
Min index in gt_box_sem_cls_label: 0
Max index in gt_box_sem_cls_label: 51
Min index in gt_box_sem_cls_label: 0
Max index in gt_box_sem_cls_label: 51
Min index in gt_box_sem_cls_label: 0
Epoch [0/720]; Iter [20/799200]; Loss 61.07; LR 2.00e-06; Iter time 4.16; ETA 38 days, 11:35:56; Mem 12775.11MB
Max index in gt_box_sem_cls_label: 51
Min index in gt_box_sem_cls_label: 0
Max index in gt_box_sem_cls_label: 51
Min index in gt_box_sem_cls_label: 0
Max index in gt_box_sem_cls_label: 51
Min index in gt_box_sem_cls_label: 0
Max index in gt_box_sem_cls_label: 51
Min index in gt_box_sem_cls_label: 0
Max index in gt_box_sem_cls_label: 51
Min index in gt_box_sem_cls_label: 0
Max index in gt_box_sem_cls_label: 51
Min index in gt_box_sem_cls_label: 0
Max index in gt_box_sem_cls_label: 51
Min index in gt_box_sem_cls_label: 0
Max index in gt_box_sem_cls_label: 51
Min index in gt_box_sem_cls_label: 0
Max index in gt_box_sem_cls_label: 51
Min index in gt_box_sem_cls_label: 0
Max index in gt_box_sem_cls_label: 51
Min index in gt_box_sem_cls_label: 0
Max index in gt_box_sem_cls_label: 51
Min index in gt_box_sem_cls_label: 0
Max index in gt_box_sem_cls_label: 51
Min index in gt_box_sem_cls_label: 0
Max index in gt_box_sem_cls_label: 51
Min index in gt_box_sem_cls_label: 0
Max index in gt_box_sem_cls_label: 51
Min index in gt_box_sem_cls_label: 0
Max index in gt_box_sem_cls_label: 51
Min index in gt_box_sem_cls_label: 0
Max index in gt_box_sem_cls_label: 51
Min index in gt_box_sem_cls_label: 0
Max index in gt_box_sem_cls_label: 51
Min index in gt_box_sem_cls_label: 0
Max index in gt_box_sem_cls_label: 51
Min index in gt_box_sem_cls_label: 0
Max index in gt_box_sem_cls_label: 51
Min index in gt_box_sem_cls_label: 0
Max index in gt_box_sem_cls_label: 51
Min index in gt_box_sem_cls_label: 0
Max index in gt_box_sem_cls_label: 51
Min index in gt_box_sem_cls_label: 0
Max index in gt_box_sem_cls_label: 51
Min index in gt_box_sem_cls_label: 0
Max index in gt_box_sem_cls_label: 51
Min index in gt_box_sem_cls_label: 0
Max index in gt_box_sem_cls_label: 51
Min index in gt_box_sem_cls_label: 0
Max index in gt_box_sem_cls_label: 51
Min index in gt_box_sem_cls_label: 0
Max index in gt_box_sem_cls_label: 51
Min index in gt_box_sem_cls_label: 0
Max index in gt_box_sem_cls_label: 51
Min index in gt_box_sem_cls_label: 0
Max index in gt_box_sem_cls_label: 51
Min index in gt_box_sem_cls_label: 0
Max index in gt_box_sem_cls_label: 51
Min index in gt_box_sem_cls_label: 0
Max index in gt_box_sem_cls_label: 51
Min index in gt_box_sem_cls_label: 0
Max index in gt_box_sem_cls_label: 51
Min index in gt_box_sem_cls_label: 0
Max index in gt_box_sem_cls_label: 51
Min index in gt_box_sem_cls_label: 0
Max index in gt_box_sem_cls_label: 51
Min index in gt_box_sem_cls_label: 0
Max index in gt_box_sem_cls_label: 51
Min index in gt_box_sem_cls_label: 0
Max index in gt_box_sem_cls_label: 51
Min index in gt_box_sem_cls_label: 0
Max index in gt_box_sem_cls_label: 51
Min index in gt_box_sem_cls_label: 0
Max index in gt_box_sem_cls_label: 51
Min index in gt_box_sem_cls_label: 0
Max index in gt_box_sem_cls_label: 51
Min index in gt_box_sem_cls_label: 0
Max index in gt_box_sem_cls_label: 51
Min index in gt_box_sem_cls_label: 0
Max index in gt_box_sem_cls_label: 51
Min index in gt_box_sem_cls_label: 0
Max index in gt_box_sem_cls_label: 51
Min index in gt_box_sem_cls_label: 0
Max index in gt_box_sem_cls_label: 51
Min index in gt_box_sem_cls_label: 0
Max index in gt_box_sem_cls_label: 51
Min index in gt_box_sem_cls_label: 0
Max index in gt_box_sem_cls_label: 51
Min index in gt_box_sem_cls_label: 0
